# CS6650 Assignment 4

TAs: This will be a group assignment

## Distributed Databases using Replication

This cuts to heart of distributed systems – how to work with the CAP theorem in distributed systems. 
This is a difficult assignment, so start early.


## Key-Value Service

You will write an in-memory Key-Value store as the foundation for a Leader-Follower database.
Essentially this service is a hashtable-as-a-service.
It has two very simple endpoints:
1. put(Key:string, Value: string)  stores the Value under that key. The empty string is a valid value.
2. get(Key:String)  returns the value (and 200) if the Key is present, otherwise return 404

The KV service does NOT need to persist any data to the file system; all values stored when the service shuts down.

You will need to create a Dockerfile just like you did for Assignment 1.

Each microservice is connected to exactly one KV service.
The microservice stores its transactions in its KV service.

THe KV service should add a 10mS delay when "writing,"  and a 5mS delay when reading.

### Leader-Follower without Quorums
Add the necessary endpoints (and the Java code that implements them) so that the cluster can be easily configured to
for N, R, and W (as defined in lecture 7). 
Leader-Follower with N == 5, i.e. one Leader and two Followers.
All writes go to the leader, which replicates the data to all other instances.
Therefore, your load-test client needs to know the IP address of the Leader and only send writes to it.
Reads can go to any instance.

Replication Strategies:
1. W=7 and R=1, meaning that every instance must be updated before the Leader responds to the client. A read only needs to read the value on the receiving istance
2. W=1 and R=7, meaning that only the Leader needs to be updated before responding, and the other nodes are scheduled to be updated. A read must fetch teh data from every node and return the most recent version.
3. Use a quorum of R==4, W==4. Reads return the most recent versio

A Read coordinator also fetches the data from R-1 other nodes. It returns the most recent version of that Key (hint, you
will need timestamps).



### SLA Resiliency Testing
*ToDo* Probably not included. Only include this if the Leader-Follower database is
very easy to implement.

Add an extra `set_reliability` endpoint to the KV cluster service to set its behaviour.
A cluster can operate in one of several modes, to simulate network issues:
1. Perfect. The default. Works as designed. No bugs.
2. Dead - ignore all requests.
3. Form a new subgroup – simulate a network partition

`set_reliability( mode, sub_group: list[IPaddress])`

The form a new subgroup mode ncauses the node to only answer or send requests to that subset.

The Distributed Dataase service will always listen to the Reliability endpoint and change its behavior as requested.

While the service is running, simulate the following and show that the databases behaves as requested, and that RAFT elects
a new leader if required.

**Note to TAs.** How do we grade this? Have them take a screen video of them sending control commands to the Reliability
Endpoint and then poll the Status endpoint to confirm correct behaviour?


### Load Testing

Modify your load test client from Assignment 2 to explore the read-write performance of the various databases. 
In order to produce data for stale reads and trigger the "return the most recent value" logic,
you must ensure that your load test generator produces data that is local,
i.e. reads and writes occur to the same key clustered closely together
in time.
You can either simply use a smaller number of keys (easy) or a more complex algorithm that clusters reads and writes
to the same key, but still has a large number of keys.

Perform load-test runs with the following read-write ratios and record your results:

| Writes | Reads |
|--------|-------|
| 1%     | 99%   |
| 10%    | 90%   |
| 50%    | 50%   |
| 90%    | 10%   |

For each run record the
1. Latency for each request.
For the Leader-Follower database,
2. record the number of stale reads
3. (you will need to track the version of the data in your client as well to detect staleness).

Create graphs of your results that show the:
1. Distribution of latency for reads and separately for writes.
These graphs should show any "long tail," if it exists.
2. Distribution of the time intervals between reading and writing the same key,
as generated by your toad-tester.

Discuss your results. How does your test generator work? How does it guarantee that reads and writes frequently occur on the same Key?

## Submission Requirements and Grading

1. All Code and configurations in a Khoury git repository.  30 points total
   1. Java code for the disrtibuted database
   2. Java code for the load tester
   2. Dockerfiles
   3. Spring Boot configuration
2. A PDF report with 20 points toal
   1. URL of the git repo.  1 point
   2. Graphs of Time Latencies: for each of the 4 read-write ratios:
      5. Reads
      6. Writes
      7. Time interval between reads and writes of the same key.
   3. Discussion of those results.
   Which type of Leader-Follower does best with each read/write ratio?
   Do not just decsribe the graps in English.






